<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />


<meta name="author" content="Guillermo Pachón" />

<meta name="date" content="2016-11-10" />

<title>Practical Machine Learning Course Project</title>

<script src="pml_cp_files/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="pml_cp_files/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="pml_cp_files/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="pml_cp_files/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="pml_cp_files/bootstrap-3.3.5/shim/respond.min.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<link rel="stylesheet"
      href="pml_cp_files/highlight/default.css"
      type="text/css" />
<script src="pml_cp_files/highlight/highlight.js"></script>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs && document.readyState && document.readyState === "complete") {
   window.setTimeout(function() {
      hljs.initHighlighting();
   }, 0);
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>


</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
button.code-folding-btn:focus {
  outline: none;
}
</style>



<div class="container-fluid main-container">

<!-- tabsets -->
<script src="pml_cp_files/navigation-1.1/tabsets.js"></script>
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->






<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Practical Machine Learning Course Project</h1>
<h4 class="author"><em>Guillermo Pachón</em></h4>
<h4 class="date"><em>November 10, 2016</em></h4>

</div>

<div id="TOC">
<ul>
<li><a href="#summary">1. Summary</a><ul>
<li><a href="#background">1.1 Background</a></li>
<li><a href="#data">1.2 Data</a></li>
<li><a href="#project">1.3 Project</a></li>
</ul></li>
<li><a href="#exploring-the-data">Exploring the Data</a></li>
<li><a href="#fitting-models">Fitting Models</a></li>
<li><a href="#model-selection">Model Selection</a></li>
<li><a href="#prediction">Prediction</a></li>
<li><a href="#conclussions">Conclussions</a></li>
</ul>
</div>

<div id="summary" class="section level2">
<h2>1. Summary</h2>
<div id="background" class="section level3">
<h3>1.1 Background</h3>
<p>Using devices such as <em>Jawbone Up</em>, <em>Nike FuelBand</em>, and <em>Fitbit</em> it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement – a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how <em>much</em> of a particular activity they do, but they rarely quantify <em>how well they do it</em>. In this project, my goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways.</p>
<p>More information is available from the website here: <a href="http://groupware.les.inf.puc-rio.br/har" class="uri">http://groupware.les.inf.puc-rio.br/har</a> (see the section on the Weight Lifting Exercise Dataset).</p>
</div>
<div id="data" class="section level3">
<h3>1.2 Data</h3>
<p>The training data for this project are available here:</p>
<p><a href="https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv" class="uri">https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv</a></p>
<p>The test data are available here:</p>
<p><a href="https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv" class="uri">https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv</a></p>
<p>The data for this project come from Groupware Technologies - Human Activity Recognition, Weight Lifting Exercises Dataset: <a href="http://groupware.les.inf.puc-rio.br/har" class="uri">http://groupware.les.inf.puc-rio.br/har</a></p>
</div>
<div id="project" class="section level3">
<h3>1.3 Project</h3>
<p>The goal of this project is to predict the manner in which they did the exercise. This is the “classe” variable in the training set. I may use any of the other variables to predict with.</p>
<p>The following report will describe how built the model, how used cross validation, what think the expected out of sample error is, and why I made the choices I did. I will also use the prediction model to predict 20 different test cases.</p>
</div>
</div>
<div id="exploring-the-data" class="section level2">
<h2>Exploring the Data</h2>
<p>We need to load required packages and set parallel options for improved performance.</p>
<pre class="r"><code># Required packages
library(RCurl); library(caret); library(relaxo); library(parallel); library(doParallel); library(reshape2);</code></pre>
<pre class="r"><code># Set parallel options
cluster &lt;- makeCluster(detectCores() - 1) # Leave 1 for OS
registerDoParallel(cluster)</code></pre>
<p>Load the data from the provided files.</p>
<pre class="r"><code>training &lt;- read.csv(file=&quot;pml-training.csv&quot;, na.strings = c(&quot;NA&quot;, &quot;#DIV/0!&quot;))
testing &lt;- read.csv(file=&quot;pml-testing.csv&quot;, na.strings = c(&quot;NA&quot;, &quot;#DIV/0!&quot;))

paste(&quot;TRAINING: Rows: &quot;, dim(training)[1], &quot;. Columns: &quot;, dim(training)[2], &quot;.&quot;, sep = &quot;&quot;)</code></pre>
<pre><code>## [1] &quot;TRAINING: Rows: 19622. Columns: 160.&quot;</code></pre>
<p>Now we will do some exploration and make some analisys. Using the information in <em>training</em> we will create the training and test data to test some prediction methods.</p>
<p>But, before the analisys, some cleaning work sholud be made in training data. Several columns contains only NA values making nearly imposible to validate the methods with the training data as is. We will remove covariates with more than 80% missing values.</p>
<pre class="r"><code># Columns contains only NA values, so that columns will be removed
training.mNA &lt;- sapply(colnames(training), function(x) if(sum(is.na(training[, x])) &gt; 0.8*nrow(training)){return(T)}else{return(F)})
training &lt;- training[, !training.mNA]

# Create partitions for train and test
set.seed(83538)
inTrain &lt;- createDataPartition(training[,1], p = 0.5, list = FALSE)
trainDF &lt;- training[inTrain,]
testDF &lt;- training[-inTrain,]</code></pre>
</div>
<div id="fitting-models" class="section level2">
<h2>Fitting Models</h2>
<p>To make an automated analisys, create a function to test some methods and try to identify the one that gets best results (Accuracy).</p>
<pre class="r"><code># Function testModel return the Accuracy from the confusionMatrix.
testModel &lt;- function(tr, ts, m = &quot;lm&quot;, usePCA = FALSE) {
  preProc = NULL
  if (usePCA) { preProc = &quot;pca&quot; }
  mFit &lt;- train(classe ~ ., method = m, data = tr, preProcess = preProc, trControl = fitControl)
  cMat &lt;- confusionMatrix(ts$classe, predict(mFit, newdata = ts))
  Accuracy = round(cMat$overall[[1]], 6)
  Accuracy
}</code></pre>
<p>In all cases, test the classiﬁer with 10-fold cross-validation.</p>
<pre class="r"><code>fitControl &lt;- trainControl(method = &quot;cv&quot;, number = 10, allowParallel = TRUE)</code></pre>
<p>Test the following models:</p>
<ul>
<li>CART</li>
<li>Random Forest</li>
<li>Stochastic Gradient Boosting</li>
<li>Naive Bayes</li>
</ul>
<p>For every method tested, the testModel funcion is called, first without and then with a Principal Components Analisys.</p>
<p>From the analisys we get the following numbers:</p>
<pre><code>##   Method Accuracy   PCA
## 1  rpart 0.661672 FALSE
## 2     rf 0.999796 FALSE
## 3    gbm 0.999694 FALSE
## 4     nb  0.86157 FALSE
## 5  rpart 0.285015  TRUE
## 6     rf 0.982977  TRUE
## 7    gbm  0.93792  TRUE
## 8     nb  0.81998  TRUE</code></pre>
</div>
<div id="model-selection" class="section level2">
<h2>Model Selection</h2>
<p>From the model analisys we get that the best method to estimate the outcome is Random Forest (Accuracy: 0.999796) or Stochastic Gradient Boosting (Accuracy: 0.999694) so we will continue with <strong>Random Forest</strong>.</p>
<p>Now using the model selected, use the leave-one-subject-out test in order to measure whether our classiﬁer trained for some subjects is still useful for a new subject.</p>
<pre class="r"><code># Partition the data by user_name to leave-one-subject-out test
unLvls &lt;- levels(training$user_name)
# Training sets removing one user
trnUN1 &lt;- training[training$user_name != unLvls[1], ]
trnUN2 &lt;- training[training$user_name != unLvls[2], ]
trnUN3 &lt;- training[training$user_name != unLvls[3], ]
trnUN4 &lt;- training[training$user_name != unLvls[4], ]
trnUN5 &lt;- training[training$user_name != unLvls[5], ]
trnUN6 &lt;- training[training$user_name != unLvls[6], ]
# Tesing sets using the user removed in training
tstUN1 &lt;- training[training$user_name == unLvls[1], ]
tstUN2 &lt;- training[training$user_name == unLvls[2], ]
tstUN3 &lt;- training[training$user_name == unLvls[3], ]
tstUN4 &lt;- training[training$user_name == unLvls[4], ]
tstUN5 &lt;- training[training$user_name == unLvls[5], ]
tstUN6 &lt;- training[training$user_name == unLvls[6], ]

# Generate models for train data leaving one user at a time
leaveSbj1Fit &lt;- train(classe ~ ., method = &quot;rf&quot;, data = trnUN1, trControl = fitControl)
leaveSbj2Fit &lt;- train(classe ~ ., method = &quot;rf&quot;, data = trnUN2, trControl = fitControl)
leaveSbj3Fit &lt;- train(classe ~ ., method = &quot;rf&quot;, data = trnUN3, trControl = fitControl)
leaveSbj4Fit &lt;- train(classe ~ ., method = &quot;rf&quot;, data = trnUN4, trControl = fitControl)
leaveSbj5Fit &lt;- train(classe ~ ., method = &quot;rf&quot;, data = trnUN5, trControl = fitControl)
leaveSbj6Fit &lt;- train(classe ~ ., method = &quot;rf&quot;, data = trnUN6, trControl = fitControl)
# Generate confusion matrix for each model
leaveSbj1ConfcMat &lt;- confusionMatrix(tstUN1$classe, predict(leaveSbj1Fit, newdata = tstUN1))
leaveSbj2ConfcMat &lt;- confusionMatrix(tstUN2$classe, predict(leaveSbj2Fit, newdata = tstUN2))
leaveSbj3ConfcMat &lt;- confusionMatrix(tstUN3$classe, predict(leaveSbj3Fit, newdata = tstUN3))
leaveSbj4ConfcMat &lt;- confusionMatrix(tstUN4$classe, predict(leaveSbj4Fit, newdata = tstUN4))
leaveSbj5ConfcMat &lt;- confusionMatrix(tstUN5$classe, predict(leaveSbj5Fit, newdata = tstUN5))
leaveSbj6ConfcMat &lt;- confusionMatrix(tstUN6$classe, predict(leaveSbj6Fit, newdata = tstUN6))
# Generate accuracy matrix for each model
lvSbj1AccTb &lt;- leaveSbj1ConfcMat$table / summary(tstUN1$classe)
lvSbj2AccTb &lt;- leaveSbj2ConfcMat$table / summary(tstUN2$classe)
lvSbj3AccTb &lt;- leaveSbj3ConfcMat$table / summary(tstUN3$classe)
lvSbj4AccTb &lt;- leaveSbj4ConfcMat$table / summary(tstUN4$classe)
lvSbj5AccTb &lt;- leaveSbj5ConfcMat$table / summary(tstUN5$classe)
lvSbj6AccTb &lt;- leaveSbj6ConfcMat$table / summary(tstUN6$classe)
# Generate summed and averaged accuracy matrix
listMatrix &lt;- list(lvSbj1AccTb, lvSbj2AccTb, lvSbj3AccTb, lvSbj4AccTb, lvSbj5AccTb, lvSbj6AccTb)
averageMatrix &lt;- Reduce(&#39;+&#39;, listMatrix) / 6</code></pre>
<p>The results are clear in the following plot:</p>
<pre class="r"><code># Transform matrix into data.frame
avgM &lt;- melt(averageMatrix)
# Plot results
g &lt;- ggplot(avgM, aes(Reference, Prediction)) + labs(title = &quot;Averaged Confission Matrix for leave-one-out-test&quot;)
g &lt;- g + geom_tile(aes(fill = value), colour = &quot;white&quot;)
g &lt;- g + geom_text(aes(label= ifelse(value == 0, &quot;&quot;, round(value, 4))), color = &quot;black&quot;, size = 4)
g &lt;- g + scale_fill_gradient(low = &quot;white&quot;, high = &quot;steelblue&quot;)
g</code></pre>
<p style="width: 100%; text-align: center;"><img src="pml_cp_files/figure-html/unnamed-chunk-9-1.png" alt="" /><!-- --></p>
<p style="width: 100%; text-align: center;"><strong>Figure</strong>. <em>Averaged Confission Matrix for leave-one-out-test</em>.<p/>
</div>
<div id="prediction" class="section level2">
<h2>Prediction</h2>
<p>Now, after verifiying the performance of the model selected, predict the <em>classe</em> for the <strong>training</strong> data.</p>
<pre class="r"><code>mFit &lt;- train(classe ~ ., method = &quot;rf&quot;, data = training, trControl = fitControl)
# final model
mFit$finalModel</code></pre>
<pre><code>## 
## Call:
##  randomForest(x = x, y = y, mtry = param$mtry) 
##                Type of random forest: classification
##                      Number of trees: 500
## No. of variables tried at each split: 41
## 
##         OOB estimate of  error rate: 0.01%
## Confusion matrix:
##      A    B    C    D    E  class.error
## A 5580    0    0    0    0 0.0000000000
## B    1 3796    0    0    0 0.0002633658
## C    0    1 3421    0    0 0.0002922268
## D    0    0    0 3216    0 0.0000000000
## E    0    0    0    0 3607 0.0000000000</code></pre>
<pre class="r"><code># prediction
prediction &lt;- predict(mFit, testing)

# De-register parallel processing cluster
stopCluster(cluster)</code></pre>
<p>So, the predicted 20 values for testing are: <code>A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A</code>.</p>
</div>
<div id="conclussions" class="section level2">
<h2>Conclussions</h2>
<ul>
<li><p>We use Random Forests as prediction method with 10-fold cross-validation. This method give us a 0.999796 Accuracy.</p></li>
<li><p>The final random forests model contains 500 trees with 41 variables tried at each split.</p></li>
<li><p>Estimated out of sample error rate for the random forests model is 0.01% as reported by the final model.</p></li>
<li><p>Also to test the efectivity with new subjects we use the Leave One Out Cross Validation (training and test sets leaving one subject in the test). This test gave the following Accuracy by class: (A) 99.7%, (B) 90.12%, (C) 88.51%, (D) 97.38%, (E) 99.63%.</p></li>
<li><p>The execution of the train function for the prediction methods is very highly procesing consuming, the previus analisys took almost two hours to complete (using parallel options for improved performance).</p></li>
</ul>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
$(document).ready(function () {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
